{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "6e7b1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder / Decoder\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, num_modules=0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(num_modules):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size=kernel_size),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layers in self.layers:\n",
    "            x = layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SDLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size=kernel_size, stride = stride)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "class SDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides=[]):\n",
    "        super().__init__()\n",
    "        self.low_freq_block = nn.Sequential(\n",
    "            SDLayer(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size, stride=(1, strides[0])),\n",
    "            nn.GELU(),\n",
    "            ConvModule(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size, num_modules=3)\n",
    "        )\n",
    "        self.mid_freq_block = nn.Sequential(\n",
    "            SDLayer(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size, stride=(1, strides[1])),\n",
    "            nn.GELU(),\n",
    "            ConvModule(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size, num_modules=2)\n",
    "        )\n",
    "        self.high_freq_block = nn.Sequential(\n",
    "            SDLayer(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size, stride=(1, strides[2])),\n",
    "            nn.GELU(),\n",
    "            ConvModule(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size, num_modules=1)\n",
    "        )\n",
    "\n",
    "        self.last_conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        ls = int(W * 0.175)\n",
    "        ms = int(W * (0.175 + 0.392))\n",
    "\n",
    "        x_low = x[..., :ls]     # (B, C, H, 0:ls)\n",
    "        x_mid = x[..., ls:ms]  # (B, C, H, ls:ms)\n",
    "        x_high = x[..., ms:]   # (B, C, H, ms:)\n",
    "\n",
    "        l = self.low_freq_block(x_low)\n",
    "        m = self.mid_freq_block(x_mid)\n",
    "        h = self.high_freq_block(x_high)\n",
    "    \n",
    "        s = torch.concat([l, m, h], dim=3)\n",
    "        e = self.last_conv(s)\n",
    "\n",
    "        return s, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "4bc3af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Separation Network\n",
    "\n",
    "class FConv(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=5, groups=8):\n",
    "        super().__init__()\n",
    "        self.blk = nn.Sequential(\n",
    "            Rearrange('b c h f -> b h f c'),\n",
    "            nn.LayerNorm(in_channels),\n",
    "            Rearrange('b h f c -> b c (h f)'),\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = in_channels, kernel_size=kernel_size,groups=groups, padding='same'),\n",
    "            nn.PReLU(in_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.blk(x)\n",
    "    \n",
    "class FullBandLinearModule(nn.Module):\n",
    "    def __init__(self, dim_hidden, dim_squeeze, num_freqs=3):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim_hidden)\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_squeeze),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.full = nn.Linear(num_freqs, num_freqs)\n",
    "        self.unsqueeze = nn.Sequential(\n",
    "            nn.Linear(dim_squeeze, dim_hidden),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.squeeze(x)\n",
    "        x = rearrange(x, 'b c f -> b f c')\n",
    "        x = self.full(x)\n",
    "        x = rearrange(x, 'b f c -> b c f')\n",
    "        x = self.unsqueeze(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class CrossBandBlock(nn.Module):\n",
    "    def __init__(self, dim_hidden, dim_squeeze, num_freqs):\n",
    "        super().__init__()\n",
    "        self.fconv0 = FConv(dim_hidden)\n",
    "        self.fblm = FullBandLinearModule(dim_hidden, dim_squeeze, num_freqs)\n",
    "        self.fconv1 = FConv(dim_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, F = x.size()\n",
    "        f0 = self.fconv0(x)\n",
    "        f0 = rearrange(f0, 'b c (h f) -> (b h) f c', b=B, h=H)\n",
    "\n",
    "        fblm = self.fblm(f0) + f0\n",
    "        fblm = rearrange(fblm, '(b h) f c -> b c h f', b=B)\n",
    "        \n",
    "        f1 = self.fconv1(fblm)  \n",
    "        f1 = rearrange(f1, 'b c (h f) -> b c h f', h=H) + fblm\n",
    "\n",
    "        return f1\n",
    "        \n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim_hidden, num_heads=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim_hidden)\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim = dim_hidden, num_heads=num_heads, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = self.norm(x)\n",
    "        x, attn = self.mhsa(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "class TConvFFN(nn.Module):\n",
    "    def __init__(self, dim_hidden, dim_ffn, kernel_size=3, groups=8, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.norm0 = nn.LayerNorm(dim_hidden)\n",
    "        self.blk0 = nn.Sequential(\n",
    "            nn.Linear(in_features=dim_hidden, out_features=dim_ffn),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.blk1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=dim_ffn, out_channels=dim_ffn, groups=groups, kernel_size=kernel_size, padding='same'),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(in_channels=dim_ffn, out_channels=dim_ffn, groups=groups, kernel_size=kernel_size, padding='same'),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=dim_ffn),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(in_channels=dim_ffn, out_channels=dim_ffn, groups=groups, kernel_size=kernel_size, padding='same'),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.blk2 = nn.Sequential(\n",
    "            nn.Linear(in_features = dim_ffn, out_features = dim_hidden),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm0(x)\n",
    "        x = self.blk0(x)\n",
    "        x = rearrange(x, 'b f c -> b c f')\n",
    "        x = self.blk1(x)\n",
    "        x = rearrange(x, 'b c f -> b f c')\n",
    "        x = self.blk2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class NarrowBandBlock(nn.Module):\n",
    "    def __init__(self, dim_hidden, dim_ffn, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(dim_hidden=dim_hidden, num_heads=num_heads)\n",
    "        self.ffn = TConvFFN(dim_hidden=dim_hidden, dim_ffn=dim_ffn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = rearrange(x, 'b c h f -> (b h) f c')\n",
    "        x, attn = self.mhsa(x)\n",
    "        x = self.ffn(x)\n",
    "        x = rearrange(x, '(b h) f c -> b c h f', b=B)\n",
    "        return x        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5527d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion Network\n",
    "\n",
    "class LinearGate(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2, padding='same')\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Linear()\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2, padding='same')\n",
    "    \n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d()\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.pwconv = nn.Conv2d(kernel_size=1)\n",
    "\n",
    "class CMHSA(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024, num_heads=4):\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim = embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        #self.q_dim = \n",
    "        self.conv_q = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2, padding='same'),\n",
    "            nn.PReLU(),\n",
    "            nn.LayerNorm()\n",
    "        )\n",
    "        self.conv_k = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2, padding='same'),\n",
    "            nn.PReLU(),\n",
    "            nn.LayerNorm()\n",
    "        )\n",
    "        self.conv_v = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2, padding='same'),\n",
    "            nn.PReLU(),\n",
    "            nn.LayerNorm()\n",
    "        )\n",
    "        self.conv_last = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2, padding='same'),\n",
    "            nn.PReLU(),\n",
    "            nn.LayerNorm()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(x)\n",
    "        v = self.conv_v(x)\n",
    "\n",
    "\n",
    "class CSAFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.cmhsa = nn.MultiheadAttention(embed_dim = embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.upscale = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        x += g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "32f51daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "blk = SDBlock(in_channels=3, out_channels=16, kernel_size=1,strides=[1,4,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "5875445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lib\n",
    "\n",
    "samp = torch.randn((2,4096)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ffd36f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2049, 5)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp = torch.randn((2,4096)).numpy()\n",
    "lib.stft(\n",
    "    y= samp, n_fft=4096, hop_length = 1024, win_length=4096,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "04cf6b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "33c30547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.7"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17.5+39.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a49a392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2352.0"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6000 * 0.392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "810ab184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606.4160000000002"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4098 * 0.392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "72f52978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1774.434"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4098 * 0.433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789f2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093b841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "bb4ee79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, e =blk(torch.randn((2, 3, 2049*2, 5*2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbb = CrossBandBlock(16, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "4cc05319",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cbb(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "b8ce1aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 4098, 3])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "e4fb70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbb = NarrowBandBlock(16, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "2dbcfcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 4098, 3])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbb(out).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
